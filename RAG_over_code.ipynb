{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7CF3m0KiDhS3EQT02m4ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takiguchi-yu/google-colab/blob/main/RAG_over_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IfwQy7OMHUdd"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain GitPython\n",
        "\n",
        "# Set env var OPENAI_API_KEY or load from a .env file\n",
        "# import dotenv\n",
        "\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from langchain.text_splitter import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import LanguageParser\n",
        "import os\n",
        "\n",
        "# Git Clone\n",
        "repo_path = \"./repo\"\n",
        "if not os.path.exists(repo_path):\n",
        "  Repo.clone_from(\"https://github.com/takiguchi-yu/cognito-google-federation-example\", to_path=repo_path)\n",
        "else:\n",
        "  repo = Repo(repo_path)\n",
        "  repo.remotes[0].pull()\n",
        "\n"
      ],
      "metadata": {
        "id": "heCFVTQIKzWN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path,\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\", \".js\", \".ts\", \".tsx\", \".css\", \".php\", \".md\", \".json\", \".html\"],\n",
        "    exclude=[\"**/.git\"],\n",
        "    parser=LanguageParser()\n",
        ")\n",
        "docs = loader.load()\n",
        "len(docs)\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdl7uJkPCAn_",
        "outputId": "38c31eb1-fef0-45d2-8786-0514edf79827"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='## Overview\\n\\n```mermaid\\nsequenceDiagram\\n    participant Frontend\\n    participant Backend\\n    participant Cognito\\n    participant Google\\n    Frontend->>Backend: /auth/login<br>Request unique login URL (Cognito Hosted UI)\\n    Backend->>Cognito: /oauth2/authorize<br>Request unique login URL (Cognito Hosted UI)\\n    Cognito->>Backend: Return unique login URL (Cognito Hosted UI)\\n    Backend->>Frontend: Return unique login URL (Cognito Hosted UI)\\n    Frontend->>Cognito: Redirect to login URL (Cognito Hosted UI)\\n    Cognito->>Cognito: Check internally if user already authenticated with Google account recently\\n    Cognito->>Google: If authorization expired or never happened,<br/> redirect to Google Hosted UI for authorization.<br/>User enters Google credentials and approves Cognito to read his Google email.\\n    Google->>Cognito: Internal redirect. <br/>Provisions the Google user to Cognito User Pool and map fields like \"email\" to Cognito user fields\\n    Cognito->>Frontend: Redirect with one-time AuthorizationCode in query parameter\\n    Frontend->>Backend: /auth/token & /user<br />Pass one-time AuthorizationCode<br/>Request long lived credentials (IdToken, AccessToken, RefreshToken)\\n    Backend->>Cognito: Exchange AuthorizationCode for long lived credentials\\n    Backend->>Frontend: Return long lived credentials\\n    Frontend->>Frontend: Store long lived credentials<br/>(Cookie, LocalStorage, IndexedDB, etc)\\n    Frontend->>Backend: Make calls to authenticated resources using long lived credentials\\n```\\n\\n## 技術スタック\\n\\n- Frontend\\n  - language: typescript\\n  - styling: tailwindcss\\n  - library: react\\n  - build tool: vite\\n- Backend\\n  - language: nodejs(typescript)\\n- infrastructure\\n  - AWS\\n\\n## AWS 環境構築\\n\\n- [AWS 環境構築](./readme-docs/environment.md)\\n\\n## 参考情報\\n\\n- 参考にしたソースコード: https://github.com/awesome-cdk/cognito-google-federation-example\\n- ユーザープールのフェデレーションエンドポイント: https://docs.aws.amazon.com/ja_jp/cognito/latest/developerguide/federation-endpoints.html\\n- ユーザープールにソーシャルサインインを追加する: https://docs.aws.amazon.com/ja_jp/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-social-idp.html\\n', metadata={'source': 'repo/README.md'})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=200, chunk_size=2048)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)\n",
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzgekOvoNWiZ",
        "outputId": "aa7b2f6e-e3d9-453a-b292-c526ebf76811"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='## Overview\\n\\n```mermaid\\nsequenceDiagram\\n    participant Frontend\\n    participant Backend\\n    participant Cognito\\n    participant Google\\n    Frontend->>Backend: /auth/login<br>Request unique login URL (Cognito Hosted UI)\\n    Backend->>Cognito: /oauth2/authorize<br>Request unique login URL (Cognito Hosted UI)\\n    Cognito->>Backend: Return unique login URL (Cognito Hosted UI)\\n    Backend->>Frontend: Return unique login URL (Cognito Hosted UI)\\n    Frontend->>Cognito: Redirect to login URL (Cognito Hosted UI)\\n    Cognito->>Cognito: Check internally if user already authenticated with Google account recently\\n    Cognito->>Google: If authorization expired or never happened,<br/> redirect to Google Hosted UI for authorization.<br/>User enters Google credentials and approves Cognito to read his Google email.\\n    Google->>Cognito: Internal redirect. <br/>Provisions the Google user to Cognito User Pool and map fields like \"email\" to Cognito user fields\\n    Cognito->>Frontend: Redirect with one-time AuthorizationCode in query parameter\\n    Frontend->>Backend: /auth/token & /user<br />Pass one-time AuthorizationCode<br/>Request long lived credentials (IdToken, AccessToken, RefreshToken)\\n    Backend->>Cognito: Exchange AuthorizationCode for long lived credentials\\n    Backend->>Frontend: Return long lived credentials\\n    Frontend->>Frontend: Store long lived credentials<br/>(Cookie, LocalStorage, IndexedDB, etc)\\n    Frontend->>Backend: Make calls to authenticated resources using long lived credentials\\n```\\n\\n## 技術スタック\\n\\n- Frontend\\n  - language: typescript\\n  - styling: tailwindcss\\n  - library: react\\n  - build tool: vite\\n- Backend\\n  - language: nodejs(typescript)\\n- infrastructure\\n  - AWS\\n\\n## AWS 環境構築\\n\\n- [AWS 環境構築](./readme-docs/environment.md)\\n\\n## 参考情報', metadata={'source': 'repo/README.md'})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "-aPWHfQ-Wdnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ea5159-f54f-4f8a-cf31-b1461410ab2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         codellama-  55%[==========>         ]   4.09G   119MB/s    eta 20s    ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "id": "9rSr1w8sOlSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "id": "r8Ax8MxSaLHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import GPT4AllEmbeddings, LlamaCppEmbeddings, GPT4AllEmbeddings\n",
        "\n",
        "db = Chroma.from_documents(texts, embedding = GPT4AllEmbeddings())\n",
        "# db.get(where={\"source\": \"repo/aws-cdk/lib\"})  # メタデータのフィルタリング\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\", # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 8},\n",
        ")\n"
      ],
      "metadata": {
        "id": "dbIv0tG0Z9ld"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llm = LlamaCpp(\n",
        "    # モデルファイルへのパス\n",
        "    model_path=\"codellama-13b-instruct.Q4_K_M.gguf\",\n",
        "    # コンテキストウィンドウ内のトークン数\n",
        "    n_ctx=5000,\n",
        "    # 使用するGPUレイヤー数\n",
        "    n_gpu_layers=30,\n",
        "    # 推論のバッチサイズ\n",
        "    n_batch=512,\n",
        "    # キーと値のストレージには半精度浮動小数点を使用する\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    # コールバックを処理するためのコールバック マネージャー\n",
        "    callback_manager=callback_manager,\n",
        "    # Verbose logging\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "WanQT_x_RmAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15da540-d26d-406a-c0d9-ac5269724d7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from codellama-13b-instruct.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32016\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 16384\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "llm_load_tensors: offloading 30 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 30/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  7500.96 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5463.57 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 5000\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =   976.56 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2929.69 MiB\n",
            "llama_new_context_with_model: KV self size  = 3906.25 MiB, K (f16): 1953.12 MiB, V (f16): 1953.12 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    20.79 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   440.39 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =   430.62 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 5\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-13b-instruct-hf', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "pUFRBBZ00Y3L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is aws-cdk?\"\n",
        "result = qa(question)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "F5UZY9uf0bTg",
        "outputId": "ba9d4716-9675-4685-f071-8c4dc7fc69a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is aws-cdk?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1305.75 ms\n",
            "llama_print_timings:      sample time =       5.22 ms /    10 runs   (    0.52 ms per token,  1915.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1852.24 ms /   314 tokens (    5.90 ms per token,   169.52 tokens per second)\n",
            "llama_print_timings:        eval time =    3021.61 ms /     9 runs   (  335.73 ms per token,     2.98 tokens per second)\n",
            "llama_print_timings:       total time =    4935.02 ms /   323 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AWS-CDK is an open-source software development framework used to define the infrastructure resources and artifacts using a programming language such as JavaScript or TypeScript and synthesized into AWS CloudFormation stacks or AWS Cloud Development Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1305.75 ms\n",
            "llama_print_timings:      sample time =     145.60 ms /   256 runs   (    0.57 ms per token,  1758.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35858.37 ms /  3783 tokens (    9.48 ms per token,   105.50 tokens per second)\n",
            "llama_print_timings:        eval time =  123523.76 ms /   255 runs   (  484.41 ms per token,     2.06 tokens per second)\n",
            "llama_print_timings:       total time =  161393.33 ms /  4038 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The human asks what the AI thinks of aws-cdk. The AI thinks aws-cdk is an open-source software development framework used to define the infrastructure resources and artifacts using a programming language such as JavaScript or TypeScript and synthesized into AWS CloudFormation stacks or AWS Cloud Development Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1305.75 ms\n",
            "llama_print_timings:      sample time =     146.70 ms /   256 runs   (    0.57 ms per token,  1745.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4505.88 ms /   680 tokens (    6.63 ms per token,   150.91 tokens per second)\n",
            "llama_print_timings:        eval time =   96006.07 ms /   255 runs   (  376.49 ms per token,     2.66 tokens per second)\n",
            "llama_print_timings:       total time =  102384.78 ms /   935 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  AWS-CDK is an open-source software development framework used to define the infrastructure resources and artifacts using a programming language such as JavaScript or TypeScript and synthesized into AWS CloudFormation stacks or AWS Cloud Development Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment Environment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}